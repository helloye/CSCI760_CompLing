{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 5: Semantic Analysis (Part 1)\n",
    "## Introduction:\n",
    "\n",
    "For the first part of my project, I will be processing a dataset that I found online, a corpus of Amazon reviews on electrioncs. I will be running them through various NLP libraries to find the tf-idf scores of each document in a vectorized space, and will be calculating their pairwise cosine similarities to determine how each document is related to the other documents in the corpus.\n",
    "\n",
    "For this project I will be using the following programming languages/libraries:\n",
    "\n",
    "Python 2.7: https://docs.python.org/2/\n",
    "\n",
    "NLTK: https://www.nltk.org/\n",
    "\n",
    "gensim: https://radimrehurek.com/gensim/\n",
    "\n",
    "scikit-learn: http://scikit-learn.org/stable/\n",
    "\n",
    "\n",
    "## Step1:  Finding data\n",
    "\n",
    "The first step in my project was to find a sufficient dataset that I can use to start building my model from. For my project on semantic analysis, I had decided to use a set of Amazon review articles on electronics that I found at the following URL:\n",
    "\n",
    "http://jmcauley.ucsd.edu/data/amazon/\n",
    "\n",
    "Credit goes to Julian McAuley, UCSD for providing a collection of 1000K+ amazon reviews on electronics.\n",
    "\n",
    "## Step 2: Cleaning/Formatting the data\n",
    "\n",
    "After obtaining the data, I noticed it was structured in JSON format, along with other fields that were not really useful for my project (i.e reviewerName, asin product code, unixReviewTime, etc...). In order to extract the field that I was interested in (reviewText), I had decided to convert them into CSV format and store them in an array for easy access.\n",
    "\n",
    "In hindesight, I could've probably used a JSON parser to run through them, but an unintended side effect was that I realized there were some entries which were not formatted properly and/or missing reviewText fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'reviewerID', u'asin', u'reviewerName', u'helpful', u'reviewText', u'overall', u'summary', u'unixReviewTime', u'reviewTime']\n",
      "\n",
      "\n",
      "==END==\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "def allFieldsPresent(jsondata):\n",
    "    return len(jsondata.keys()) == 9\n",
    "\n",
    "#SHORT DATA\n",
    "f=open('./datasets/amazon_review_electronic_short.json','r')\n",
    "#f=open('./src/jsondata_test.json','r')\n",
    "w=open('./src/amazonReviewElectronicShortCSV.csv','w')\n",
    "\n",
    "#LONGER/ACTUAL DATA\n",
    "#f=open('./datasets/amazon_review_electronic_full.json','r')\n",
    "#w=open('./datasets/CSV_AMAZON_REVIEW_ELECTRONIC_FULL.csv','w')\n",
    "\n",
    "csvwriter = csv.writer(w)\n",
    "\n",
    "rowcount=0\n",
    "for line in f:\n",
    "    jsondata = json.loads(line)\n",
    "    if rowcount == 0:\n",
    "        header = jsondata.keys()\n",
    "        print (header)\n",
    "        csvwriter.writerow(header)\n",
    "    \n",
    "    rowcount += 1\n",
    "\n",
    "    # Only convert if all fields are present. Some docs do not have reviewerName.\n",
    "    if allFieldsPresent(jsondata):\n",
    "        csvwriter.writerow(jsondata.values())\n",
    "        \n",
    "    #if rowcount % 100000 == 0:\n",
    "    #    print ('Processing Mark:',rowcount)\n",
    "\n",
    "w.close()\n",
    "f.close()\n",
    "print ('\\n\\n==END==\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Creating the corpus\n",
    "\n",
    "To create the corpus, I basically read through the csv file and extracted the 4th index (reviewText) and stored them in a single array (with each index being a single review/document) since for the libraries that I'll be using, it required the corpus to be stored as a single array.\n",
    "\n",
    "```python\n",
    "f = open('./src/amazonReviewElectronicShortCSV.csv','r')\n",
    "\n",
    "csvdata = csv.reader(f)\n",
    "\n",
    "#Index listing\n",
    "# 0 - reviewerID\n",
    "# 1 - asin\n",
    "# 2 - reviewerName\n",
    "# 3 - helpful\n",
    "# 4 - reviewText (Use this as corpus?)\n",
    "# 5 - overall\n",
    "# 6 - summary\n",
    "# 7 - unixReviewTime\n",
    "# 8 - reviewTime\n",
    "\n",
    "#Generate corpus\n",
    "documents = []\n",
    "rowcount = 0\n",
    "for row in csvdata:\n",
    "    if rowcount > 0:\n",
    "        documents.append(row[4])\n",
    "    rowcount+=1\n",
    "    \n",
    "f.close()\n",
    "```\n",
    "\n",
    "## Step 4: Tokenize/Creating stoplist\n",
    "\n",
    "A stoplist is a list of words that we generally do not care about. Preposition words such as \"like\", \"through\", \"at\" and other words that generate unnecessary noise in our dataset are considered stoplist words. These words will need to be removed from our corpus. Fortunately, the Python nltk library readily provides for us a list of stop words that we can use by simply providing a language type parameter (in our case english), and it will return a set of stoplist words.\n",
    "\n",
    "```python\n",
    "#Remove stop words\n",
    "stoplist = set(nltk.corpus.stopwords.words('english'))\n",
    "stoplist.update(['-'])\n",
    "\n",
    "texts = [[ word for word in document.lower().split() if word not in stoplist]\n",
    "         for document in documents]\n",
    "```\n",
    "\n",
    "While checking all the words in all of our documents to see if they are in the stoplist, we can also tokenize them at the same time.\n",
    "\n",
    "## Step 5: Storing tokenized words into a dictionary\n",
    "\n",
    "Next, once we have tokenized all the words, we can store them uniquely into a dictionary. For this, we will use the gensim library to generation a Dictionary for us by supplying a list of tokenized words.\n",
    "\n",
    "```python\n",
    "#Store dictionary as binary/txt using gensim\n",
    "dictionary = gensim.corpora.Dictionary(texts)\n",
    "#dictionary.save('./dict/amazon_electronic_review.dict')\n",
    "dictionary.save_as_text('./src/dict/amazon_slectronic_review_text.txt')\n",
    "```\n",
    "Although we won't be using the dictionary/tokenized values in this part of the project, it's none the less a good resource to have, in case other libraries will require a tokenized list or a dictonary of the corpus.\n",
    "\n",
    "## Step 6: Calculating the tf-idf score\n",
    "\n",
    "To find the tf-idf score, we will use the scikitlearn library. This library will generate a tf-idf score, based upon a corpus input parameter, along with optional stoplist parameters that will remove all the unnecessary stoplist words for you automatically.\n",
    "\n",
    "Before we continue, it's important to define what the tf-idf score is. Tf-idf stands for Term Frequency - Inverse Document Frequency. It's basically the following formula:\n",
    "\n",
    "Tf(term): (frequency of a given term in a document) / (normalized over the total number of terms in the document)\n",
    "i.e:\n",
    "Given a sentence: \"This project is a very hard project\"\n",
    "Tf(project) = 2/7 = 0.285714\n",
    "\n",
    "Idf(term): log((Total number of document) / (number of document containing the term))\n",
    "i.e:\n",
    "Given these two sentences:\n",
    "A - \"This project is a very hard project\"\n",
    "B - \"I like this project\"\n",
    "Idf(project) = log(2/2) = log(1) = 0\n",
    "\n",
    "Tf-idf is simply then: Tf * Idf\n",
    "\n",
    "This score is useful as it will tell us the frequency of a particular term in the document, with respect to the number of frequencey across the entire corpus.\n",
    "\n",
    "Note* a logrithmic function is applied to the Idf calculation. This is purely for weighing up/down the result as the size of the dataset (and thus the frequency of the term) grows. This is not as noticable in our example as it only contains two documents/sentences, as opposed to a corpus of millions of lines/documents.\n",
    "\n",
    "To calculate the TfIdf score, we will use the library scikitlearn, which readly provides us a function that will give us the vectorized tf-idf score per document. We simply need to provide the corpus and an optional parameter of stoplist:\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#\n",
    "# TF-IDF Vectorizing using scikitlearn\n",
    "#\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stoplist, use_idf=True)\n",
    "V = tfidf_vectorizer.fit_transform(documents)\n",
    "```\n",
    "\n",
    "## Step 7: Finding cosine similarities\n",
    "\n",
    "Now that we have a the tf-idf score of the document in a vectorized space, we can calculate the angle between any two document to determine how close they are to each other. Recall that cos(theta) ranges from -1 to 1, and that cos(0) = 1. Thus when we compare the cosine of the difference in their angles, the closer the result is to 1, the closer in similarities their tf-idf score. To get the pairwise cosine similarity of a document to all the other documents in our corpus, scikit-learn readily provides for us the cosine_similarity function, into which we will pass two parameters. The first being the vectorized tf-idf score of the document that we would like to compare, and the second being the entire collection of tf-idf score for all the documents in the corpus for us to compare with:\n",
    "\n",
    "```python\n",
    "cs_results = cosine_similarity(V[0:3], V)\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==END==\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import gensim\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from pprint import pprint\n",
    "\n",
    "f = open('./src/amazonReviewElectronicShortCSV.csv','r')\n",
    "\n",
    "csvdata = csv.reader(f)\n",
    "\n",
    "#Index listing\n",
    "# 0 - reviewerID\n",
    "# 1 - asin\n",
    "# 2 - reviewerName\n",
    "# 3 - helpful\n",
    "# 4 - reviewText (Use this as corpus?)\n",
    "# 5 - overall\n",
    "# 6 - summary\n",
    "# 7 - unixReviewTime\n",
    "# 8 - reviewTime\n",
    "\n",
    "#Generate corpus\n",
    "documents = []\n",
    "rowcount = 0\n",
    "for row in csvdata:\n",
    "    if rowcount > 0:\n",
    "        documents.append(row[4])\n",
    "    rowcount+=1\n",
    "    \n",
    "f.close()\n",
    "\n",
    "#Remove stop words\n",
    "stoplist = set(nltk.corpus.stopwords.words('english'))\n",
    "stoplist.update(['-'])\n",
    "\n",
    "texts = [[ word for word in document.lower().split() if word not in stoplist]\n",
    "         for document in documents]\n",
    "\n",
    "\n",
    "#Store dictionary as binary/txt using gensim\n",
    "dictionary = gensim.corpora.Dictionary(texts)\n",
    "#dictionary.save('./dict/amazon_electronic_review.dict')\n",
    "dictionary.save_as_text('./src/dict/amazon_slectronic_review_text.txt')\n",
    "\n",
    "\n",
    "#\n",
    "# TF-IDF Vectorizing using scikitlearn\n",
    "#\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stoplist, use_idf=True)\n",
    "V = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "#print V[0:3]\n",
    "\n",
    "#\n",
    "# Mapping feature score to actual words in doc\n",
    "#\n",
    "document_number=0\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "feature_index = V[document_number,:].nonzero()[1]\n",
    "tfidf_scores = zip(feature_index, [V[document_number, x] for x in feature_index])\n",
    "#for word, score in [(feature_names[index], score) for (index, score) in tfidf_scores]:\n",
    "#  print str(word) + ' => ' + str(score)\n",
    "\n",
    "#\n",
    "# Calculating the pairwise cosine similarity for each document in corpus\n",
    "#\n",
    "\n",
    "cs_results = cosine_similarity(V[0:3], V)\n",
    "doc_num = 1\n",
    "for i_result in cs_results:\n",
    "#    print \"Document#: \",doc_num,'\\n'\n",
    "#    print i_result,'\\n\\n'\n",
    "    doc_num += 1\n",
    "\n",
    "\n",
    "print '\\n\\n==END==\\n\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
