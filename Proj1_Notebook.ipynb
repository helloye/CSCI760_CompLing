{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 5: Semantic Analysis (Part 1)\n",
    "## Introduction:\n",
    "\n",
    "For the first part of my project, I will be processing a dataset that I found online, a corpus of Amazon reviews on electrioncs. I will be running them through various NLP libraries to find the tf-idf scores of each document in a vectorized space, and will be calculating their pairwise cosine similarities to determine how each document is related to the other documents in the corpus.\n",
    "\n",
    "For this project I will be using the following programming languages/libraries:\n",
    "\n",
    "Python 2.7: https://docs.python.org/2/\n",
    "\n",
    "NLTK: https://www.nltk.org/\n",
    "\n",
    "gensim: https://radimrehurek.com/gensim/\n",
    "\n",
    "scikit-learn: http://scikit-learn.org/stable/\n",
    "\n",
    "\n",
    "## Step 1:  Finding data\n",
    "\n",
    "The first step in my project was to find a sufficient dataset that I can use to start building my model from. For my project on semantic analysis, I had decided to use a set of Amazon review articles on electronics that I found at the following URL:\n",
    "\n",
    "http://jmcauley.ucsd.edu/data/amazon/\n",
    "\n",
    "Credit goes to Julian McAuley, UCSD for providing a collection of 1000K+ amazon reviews on electronics.\n",
    "\n",
    "## Step 2: Cleaning/Formatting the data\n",
    "\n",
    "After obtaining the data, I noticed it was structured in JSON format, along with other fields that were not really useful for my project (i.e reviewerName, asin product code, unixReviewTime, etc...). In order to extract the field that I was interested in (reviewText), I had decided to convert them into CSV format and store them in an array for easy access.\n",
    "\n",
    "In hindesight, I could've probably used a JSON parser to run through them, but an unintended side effect was that I realized there were some entries which were not formatted properly and/or missing reviewText fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'reviewerID', u'asin', u'reviewerName', u'helpful', u'reviewText', u'overall', u'summary', u'unixReviewTime', u'reviewTime']\n",
      "\n",
      "\n",
      "==END==\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "def allFieldsPresent(jsondata):\n",
    "    return len(jsondata.keys()) == 9\n",
    "\n",
    "#SHORT DATA\n",
    "f=open('./datasets/amazon_review_electronic_short.json','r')\n",
    "#f=open('./src/jsondata_test.json','r')\n",
    "w=open('./src/amazonReviewElectronicShortCSV.csv','w')\n",
    "\n",
    "#LONGER/ACTUAL DATA\n",
    "#f=open('./datasets/amazon_review_electronic_full.json','r')\n",
    "#w=open('./datasets/CSV_AMAZON_REVIEW_ELECTRONIC_FULL.csv','w')\n",
    "\n",
    "csvwriter = csv.writer(w)\n",
    "\n",
    "rowcount=0\n",
    "for line in f:\n",
    "    jsondata = json.loads(line)\n",
    "    if rowcount == 0:\n",
    "        header = jsondata.keys()\n",
    "        print (header)\n",
    "        csvwriter.writerow(header)\n",
    "    \n",
    "    rowcount += 1\n",
    "\n",
    "    # Only convert if all fields are present. Some docs do not have reviewerName.\n",
    "    if allFieldsPresent(jsondata):\n",
    "        csvwriter.writerow(jsondata.values())\n",
    "        \n",
    "    #if rowcount % 100000 == 0:\n",
    "    #    print ('Processing Mark:',rowcount)\n",
    "\n",
    "w.close()\n",
    "f.close()\n",
    "print ('\\n\\n==END==\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample CSV output of a document:\n",
    "\n",
    "```csv\n",
    "[reviewerID,asin,reviewerName,helpful,reviewText,overall,summary,unixReviewTime,reviewTime]\n",
    "\n",
    "[AO94DHGC771SJ,0528881469,amazdnu,\"[0, 0]\",\"We got this GPS for my husband who is an (OTR) over the road trucker.  Very Impressed with the shipping time, it arrived a few days earlier than expected...  within a week of use however it started freezing up... could of just been a glitch in that unit.  Worked great when it worked!  Will work great for the normal person as well but does have the \"\"trucker\"\" option. (the big truck routes - tells you when a scale is coming up ect...)  Love the bigger screen, the ease of use, the ease of putting addresses into memory.  Nothing really bad to say about the unit with the exception of it freezing which is probably one in a million and that's just my luck.  I contacted the seller and within minutes of my email I received a email back with instructions for an exchange! VERY impressed all the way around!\",5.0,Gotta have GPS!,1370131200,\"06 2, 2013\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Creating the corpus\n",
    "\n",
    "To create the corpus, I basically read through the csv file and extracted the 4th index (reviewText) and stored them in a single array (with each index being a single review/document) since for the libraries that I'll be using, it required the corpus to be stored as a single array.\n",
    "\n",
    "```python\n",
    "f = open('./src/amazonReviewElectronicShortCSV.csv','r')\n",
    "\n",
    "csvdata = csv.reader(f)\n",
    "\n",
    "#Index listing\n",
    "# 0 - reviewerID\n",
    "# 1 - asin\n",
    "# 2 - reviewerName\n",
    "# 3 - helpful\n",
    "# 4 - reviewText (Use this as corpus?)\n",
    "# 5 - overall\n",
    "# 6 - summary\n",
    "# 7 - unixReviewTime\n",
    "# 8 - reviewTime\n",
    "\n",
    "#Generate corpus\n",
    "documents = []\n",
    "rowcount = 0\n",
    "for row in csvdata:\n",
    "    if rowcount > 0:\n",
    "        documents.append(row[4])\n",
    "    rowcount+=1\n",
    "    \n",
    "f.close()\n",
    "```\n",
    "\n",
    "## Step 4: Tokenize/Creating stoplist\n",
    "\n",
    "A stoplist is a list of words that we generally do not care about. Preposition words such as \"like\", \"through\", \"at\" and other words that generate unnecessary noise in our dataset are considered stoplist words. These words will need to be removed from our corpus. Fortunately, the Python nltk library readily provides for us a list of stop words that we can use by simply providing a language type parameter (in our case english), and it will return a set of stoplist words.\n",
    "\n",
    "```python\n",
    "#Remove stop words\n",
    "stoplist = set(nltk.corpus.stopwords.words('english'))\n",
    "stoplist.update(['-'])\n",
    "\n",
    "texts = [[ word for word in document.lower().split() if word not in stoplist]\n",
    "         for document in documents]\n",
    "```\n",
    "\n",
    "While checking all the words in all of our documents to see if they are in the stoplist, we can also tokenize them at the same time.\n",
    "\n",
    "## Step 5: Storing tokenized words into a dictionary\n",
    "\n",
    "Next, once we have tokenized all the words, we can store them uniquely into a dictionary. For this, we will use the gensim library to generation a Dictionary for us by supplying a list of tokenized words.\n",
    "\n",
    "```python\n",
    "#Store dictionary as binary/txt using gensim\n",
    "dictionary = gensim.corpora.Dictionary(texts)\n",
    "#dictionary.save('./dict/amazon_electronic_review.dict')\n",
    "dictionary.save_as_text('./src/dict/amazon_slectronic_review_text.txt')\n",
    "```\n",
    "Although we won't be using the dictionary/tokenized values in this part of the project, it's none the less a good resource to have, in case other libraries will require a tokenized list or a dictonary of the corpus.\n",
    "\n",
    "## Step 6: Calculating the tf-idf score\n",
    "\n",
    "To find the tf-idf score, we will use the scikitlearn library. This library will generate a tf-idf score, based upon a corpus input parameter, along with optional stoplist parameters that will remove all the unnecessary stoplist words for you automatically.\n",
    "\n",
    "Before we continue, it's important to define what the tf-idf score is. Tf-idf stands for Term Frequency - Inverse Document Frequency. It's basically the following formula:\n",
    "\n",
    "Tf(term): (frequency of a given term in a document) / (normalized over the total number of terms in the document)\n",
    "i.e:\n",
    "Given a sentence: \"This project is a very hard project\"\n",
    "Tf(project) = 2/7 = 0.285714\n",
    "\n",
    "Idf(term): log((Total number of document) / (number of document containing the term))\n",
    "i.e:\n",
    "Given these two sentences:\n",
    "A - \"This project is a very hard project\"\n",
    "B - \"I like this project\"\n",
    "Idf(project) = log(2/2) = log(1) = 0\n",
    "\n",
    "Tf-idf is simply then: Tf * Idf\n",
    "\n",
    "<img src=\"files/tfidf_eq.png\">\n",
    "\n",
    "This score is useful as it will tell us the frequency of a particular term in the document, with respect to the number of frequencey across the entire corpus.\n",
    "\n",
    "Note* a logrithmic function is applied to the Idf calculation. This is purely for weighing up/down the result as the size of the dataset (and thus the frequency of the term) grows. This is not as noticable in our example as it only contains two documents/sentences, as opposed to a corpus of millions of lines/documents.\n",
    "\n",
    "To calculate the TfIdf score, we will use the library scikitlearn, which readly provides us a function that will give us the vectorized tf-idf score per document. We simply need to provide the corpus and an optional parameter of stoplist:\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#\n",
    "# TF-IDF Vectorizing using scikitlearn\n",
    "#\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stoplist, use_idf=True)\n",
    "V = tfidf_vectorizer.fit_transform(documents)\n",
    "```\n",
    "#### Vectorized tf-idf score\n",
    "The output will be a vector V of the tf-idf scores for each individual document. Sample output of the first 7 terms of the first document:\n",
    "\n",
    "```\n",
    "  (0, 532)\t0.0880608061719492\n",
    "  (0, 535)\t0.09853796408314193\n",
    "  (0, 614)\t0.11644878383359814\n",
    "  (0, 869)\t0.11644878383359814\n",
    "  (0, 1060)\t0.10901512199433466\n",
    "  (0, 1293)\t0.23289756766719627\n",
    "  (0, 623)\t0.21803024398866933\n",
    "  .\n",
    "  .\n",
    "  .\n",
    "```\n",
    "In the above resulting vector, the first value is a (document index in corpus, term index in dictionary) pair, while the second value is its respective tf-idf feature score.\n",
    "\n",
    "Printing out the shape of V, we can see that it's a 99x1430 sparse matrix, where the 99 rows are our the number of documents in our corpus, with 1430 unique terms that we are storing in our dictionary for our corpus. (99 row cause one of the row was an invalid entry that I during the CSV parsing step.)\n",
    "\n",
    "```\n",
    "<99x1430 sparse matrix of type '<type 'numpy.float64'>'\n",
    "```\n",
    "\n",
    "#### Matching them to actual term\n",
    "\n",
    "By looking up the term index and mapping them to our result using the following lines of code:\n",
    "\n",
    "```python\n",
    "document_number=0\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "feature_index = V[document_number,:].nonzero()[1]\n",
    "tfidf_scores = zip(feature_index, [V[document_number, x] for x in feature_index])\n",
    "for word, score in [(feature_names[index], score) for (index, score) in tfidf_scores]:\n",
    "  print str(word) + ' => ' + str(score)\n",
    "```\n",
    "We will get the output for the first 7 terms as follow:\n",
    "\n",
    "```\n",
    "got => 0.0880608061719492\n",
    "gps => 0.09853796408314193\n",
    "husband => 0.11644878383359814\n",
    "otr => 0.11644878383359814\n",
    "road => 0.10901512199433466\n",
    "trucker => 0.23289756766719627\n",
    "impressed => 0.21803024398866933\n",
    ".\n",
    ".\n",
    ".\n",
    "```\n",
    "\n",
    "## Step 7: Finding cosine similarities\n",
    "\n",
    "Now that we have a the tf-idf score of the document in a vectorized space, we can calculate the angle between any two document to determine how close they are to each other. Recall that cos(theta) ranges from -1 to 1, and that cos(0) = 1. Thus when we compare the cosine of the difference in their angles, the closer the result is to 1, the closer in similarities their tf-idf score. To get the pairwise cosine similarity of a document to all the other documents in our corpus, scikit-learn readily provides for us the cosine_similarity function, into which we will pass two parameters. The first being the vectorized tf-idf score of the document that we would like to compare, and the second being the entire collection of tf-idf score for all the documents in the corpus for us to compare with:\n",
    "\n",
    "```python\n",
    "cs_results = cosine_similarity(V[0:3], V)\n",
    "```\n",
    "\n",
    "<img src=\"files/cosine_pic.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document#:  1 \n",
      "\n",
      "[ 1.          0.06448182  0.17047312  0.10509248  0.1618741   0.          0.\n",
      "  0.02447777  0.03198922  0.06696618  0.          0.          0.01635356\n",
      "  0.          0.04392718  0.00866927  0.03240968  0.00879309  0.\n",
      "  0.04998871  0.05426122  0.01145236  0.08385288  0.06949689  0.02494436\n",
      "  0.04321394  0.01178378  0.10896441  0.          0.05077     0.04628443\n",
      "  0.05622397  0.0429728   0.02878872  0.01505179  0.04503667  0.\n",
      "  0.00559552  0.04642412  0.00637493  0.03398488  0.03087508  0.\n",
      "  0.05252068  0.          0.04118057  0.13082372  0.07467776  0.05148969\n",
      "  0.04338703  0.03146021  0.04940529  0.02292995  0.05466849  0.01069828\n",
      "  0.03212453  0.04472577  0.03248506  0.08553808  0.01743837  0.03154781\n",
      "  0.02796502  0.04210158  0.03061191  0.12345703  0.03368483  0.02726801\n",
      "  0.0094039   0.10593499  0.03497451  0.05347856  0.03176913  0.04370887\n",
      "  0.10662     0.03737303  0.01101159  0.07845182  0.00998599  0.02682439\n",
      "  0.01561005  0.          0.03599854  0.01830357  0.          0.0203244\n",
      "  0.02333305  0.02236015  0.02515954  0.02250455  0.05643389  0.01655845\n",
      "  0.02628181  0.0592167   0.03604747  0.          0.08406248  0.0308791   0.\n",
      "  0.0262137 ] \n",
      "\n",
      "\n",
      "Document#:  2 \n",
      "\n",
      "[ 0.06448182  1.          0.3060405   0.21205275  0.23927147  0.\n",
      "  0.01477786  0.02652766  0.          0.03156802  0.          0.00630471\n",
      "  0.01361659  0.00678717  0.01210595  0.01685684  0.02319904  0.02532887\n",
      "  0.01355251  0.00635804  0.03006473  0.02215561  0.01353076  0.03674633\n",
      "  0.00705901  0.02870758  0.00348287  0.0480275   0.01393787  0.03363795\n",
      "  0.0064159   0.01897726  0.00501658  0.04649432  0.02098874  0.00779336\n",
      "  0.01492806  0.          0.06697114  0.00471469  0.05775555  0.00661289\n",
      "  0.01926835  0.0358169   0.01606827  0.00950984  0.04273367  0.00922046\n",
      "  0.02282876  0.01088824  0.0533235   0.          0.04668423  0.          0.0438368\n",
      "  0.02530859  0.03072602  0.03674759  0.04920295  0.0273909   0.04175243\n",
      "  0.01797711  0.00617626  0.00510394  0.05280776  0.01354272  0.03104999\n",
      "  0.00706778  0.01614383  0.03625555  0.0236602   0.          0.\n",
      "  0.05748463  0.04992824  0.          0.03940756  0.03196776  0.01590937\n",
      "  0.02283491  0.          0.01892277  0.          0.02462188  0.00763793\n",
      "  0.04504448  0.00982379  0.03288914  0.08819272  0.02572325  0.01308342\n",
      "  0.01766912  0.00783263  0.00433634  0.03568252  0.02453619  0.02991318\n",
      "  0.01698342  0.02562816] \n",
      "\n",
      "\n",
      "Document#:  3 \n",
      "\n",
      "[ 0.17047312  0.3060405   1.          0.33616872  0.33464399  0.00945509\n",
      "  0.00564284  0.01252984  0.01652177  0.03583484  0.          0.00581393\n",
      "  0.04999786  0.02734572  0.03106199  0.02305003  0.05205488  0.03165112\n",
      "  0.00266457  0.00960074  0.05413674  0.02538803  0.03743597  0.01956028\n",
      "  0.03971962  0.02403385  0.01753203  0.01880165  0.01871364  0.03215632\n",
      "  0.06021859  0.02223131  0.00845041  0.03413327  0.03455555  0.02723811\n",
      "  0.00893313  0.02324569  0.10200868  0.05077585  0.01170929  0.00989808\n",
      "  0.03969781  0.04643802  0.00877316  0.04651316  0.10128893  0.04257904\n",
      "  0.10376032  0.04238901  0.11492789  0.00964527  0.0248734   0.\n",
      "  0.04288057  0.04313113  0.05763381  0.04089762  0.05495649  0.01334027\n",
      "  0.04386971  0.02691557  0.04463422  0.02093645  0.0366019   0.0384559\n",
      "  0.02500237  0.03359918  0.00620684  0.03766699  0.03489362  0.01491212\n",
      "  0.01188175  0.05109765  0.01847609  0.02638375  0.08790237  0.03322949\n",
      "  0.          0.01353689  0.01461531  0.01993053  0.02892781  0.01962513\n",
      "  0.01178803  0.02064685  0.01008155  0.00716582  0.01118887  0.01734118\n",
      "  0.0221148   0.03632123  0.01497052  0.04287274  0.02132412  0.07401735\n",
      "  0.02482784  0.01432041  0.0338356 ] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==END==\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import gensim\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from pprint import pprint\n",
    "\n",
    "f = open('./src/amazonReviewElectronicShortCSV.csv','r')\n",
    "\n",
    "csvdata = csv.reader(f)\n",
    "\n",
    "#Index listing\n",
    "# 0 - reviewerID\n",
    "# 1 - asin\n",
    "# 2 - reviewerName\n",
    "# 3 - helpful\n",
    "# 4 - reviewText (Use this as corpus?)\n",
    "# 5 - overall\n",
    "# 6 - summary\n",
    "# 7 - unixReviewTime\n",
    "# 8 - reviewTime\n",
    "\n",
    "#Generate corpus\n",
    "documents = []\n",
    "rowcount = 0\n",
    "for row in csvdata:\n",
    "    if rowcount > 0:\n",
    "        documents.append(row[4])\n",
    "    rowcount+=1\n",
    "    \n",
    "f.close()\n",
    "\n",
    "#Remove stop words\n",
    "stoplist = set(nltk.corpus.stopwords.words('english'))\n",
    "stoplist.update(['-'])\n",
    "\n",
    "texts = [[ word for word in document.lower().split() if word not in stoplist]\n",
    "         for document in documents]\n",
    "\n",
    "\n",
    "#Store dictionary as binary/txt using gensim\n",
    "dictionary = gensim.corpora.Dictionary(texts)\n",
    "#dictionary.save('./dict/amazon_electronic_review.dict')\n",
    "dictionary.save_as_text('./src/dict/amazon_slectronic_review_text.txt')\n",
    "\n",
    "\n",
    "#\n",
    "# TF-IDF Vectorizing using scikitlearn\n",
    "#\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stoplist, use_idf=True)\n",
    "V = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "#print V[0:3]\n",
    "\n",
    "#\n",
    "# Mapping feature score to actual words in doc\n",
    "#\n",
    "document_number=0\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "feature_index = V[document_number,:].nonzero()[1]\n",
    "tfidf_scores = zip(feature_index, [V[document_number, x] for x in feature_index])\n",
    "#for word, score in [(feature_names[index], score) for (index, score) in tfidf_scores]:\n",
    "#  print str(word) + ' => ' + str(score)\n",
    "\n",
    "#\n",
    "# Calculating the pairwise cosine similarity for each document in corpus\n",
    "#\n",
    "\n",
    "cs_results = cosine_similarity(V[0:3], V)\n",
    "doc_num = 1\n",
    "for i_result in cs_results:\n",
    "    print ('Document#: ',doc_num,'\\n')\n",
    "    print (i_result,'\\n\\n')\n",
    "    doc_num += 1\n",
    "\n",
    "\n",
    "print ('\\n\\n==END==\\n\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
