{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 5: Semantic Analysis (Part 2)\n",
    "## Recap:\n",
    "\n",
    "In my previous presentation, I had covered the process of finding the right dataset and the importance of getting the correct type of data (labeled vs. unlabeled). I also went over the process of scrubbing and cleaning the data, process such as removing invalid entries with missing data, conversion of data format, and removal of certain stop-words (words that we do not care about) to form the corpus that we will be using in this project.\n",
    "\n",
    "I also went over some of the useful python libraries that I used to help me process my dataset. The notable ones that stood out to me were the NLTK and SciKitLearn libraries.\n",
    "\n",
    "Lastly I went over the process of analyzing my corpus and obtaiing the TF-IDF scores for each document in my corpus. This presented me with a vectorized dataset which I was then able to locate in an euclideian space, and perform a cosine similaritie comparison between each document to determine it's closeness to one another in terms their TF-IDF scores.\n",
    "\n",
    "## Part 2-1: Data Visualization\n",
    "\n",
    "Having found the data in euclidean space (TF-IDF scores of each document), I felt that it would be useful for us to be able to visualize this data in a 2/3 dimensional space. To do that, there are a few data visalization tools that we can use.\n",
    "\n",
    "#### t-SNE\n",
    "\n",
    "t-Distributed Stochastic Neighbor Embedding - From sklearn library.\n",
    " - Dimension reduction algorithm\n",
    " - Plots higher level n-dimension (n > 3) into 2 or 3 dimensional space\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2-2: Performing K-Means Clustering\n",
    "\n",
    "For the next part of the project, I had decided to perform K-Means clustering to start grouping my documents into similar topics.\n",
    "\n",
    "For this, I had decided to continue using the sklearn library, and use it's KMeans functionality to help me perform K-Mean clustering on my previously obtained TF-IDF vector of all the documents in my corpus.\n",
    "\n",
    "#### How K-Means Clustering Work?\n",
    "\n",
    "Input: takes in a set of points in euclidean space (x1...xn)\n",
    "Places initial k number of centroids (c1...ck)\n",
    "Repeat until convergence of all points onto the centroids:\n",
    "    - for each point x:\n",
    "        * Find nearest centroid\n",
    "        * Assign the point to the centroid's cluster\n",
    "    - for each cluster\n",
    "        * Assign new centroid based on the average distance of all points in the cluster\n",
    "\n",
    "\n",
    "<img src=\"files/kmeans_info.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
