{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 5: Semantic Analysis (Part 2)\n",
    "## Recap:\n",
    "\n",
    "In my previous presentation, I had covered the process of finding the right dataset and the importance of getting the correct type of data (labeled vs. unlabeled). I also went over the process of scrubbing and cleaning the data, process such as removing invalid entries with missing data, conversion of data format, and removal of certain stop-words (words that we do not care about) to form the corpus that we will be using in this project.\n",
    "\n",
    "I also went over some of the useful python libraries that I used to help me process my dataset. The notable ones that stood out to me were the NLTK and SciKitLearn libraries.\n",
    "\n",
    "Lastly I went over the process of analyzing my corpus and obtaiing the TF-IDF scores for each document in my corpus. This presented me with a vectorized dataset which I was then able to locate in an euclideian space, and perform a cosine similaritie comparison between each document to determine it's closeness to one another in terms their TF-IDF scores.\n",
    "\n",
    "## Part 2-1: Data Visualization\n",
    "\n",
    "Having found the data in euclidean space (TF-IDF scores of each document), I felt that it would be useful for us to be able to visualize this data in a 2/3 dimensional space. To do that, there are a few data visalization tools that we can use.\n",
    "\n",
    "#### t-SNE\n",
    "\n",
    "t-Distributed Stochastic Neighbor Embedding - From sklearn library.\n",
    "Developed by Geoffry Hinton and Laurens van der Maaten, it is a machine learning algorithm to help with dimensional reduction.\n",
    "\n",
    " - Dimension reduction algorithm\n",
    " - Plots higher level n-dimension (n > 3) into 2 or 3 dimensional space\n",
    " - Reducing it to a lower 2/3 dimensional space will allow us to plot it and visualize it.\n",
    " - Retains a lot of the relations of the higher dimensional graph\n",
    " \n",
    "#### Naive projection:\n",
    " \n",
    "<img src=\"files/tsne1.png\">\n",
    "\n",
    "As you can see, projecting it in a naive way will cause the points to cluster together in an unfavorable way that will cause them to lose their original properties and relative values towards one another.\n",
    "\n",
    "#### T-SNE projection:\n",
    "\n",
    "<img src=\"files/tsne2.png\">\n",
    "\n",
    "Ideally we will want the projected plots in the lower dimension to retain their original properties, which effectively allows us to view the data in a correct manner.\n",
    "\n",
    "T-SNE uses machine learning algorithm to calculate the probability of each point being in a similar cluster with the next. It finds this similarity score based on a normal Gausian distribution. After these scores are obtained, it then projects the plots onto a lower order dimension in a random mannar and try to re-adjust itself to match the distribution scores that we obtaine from above:\n",
    "\n",
    "<img src=\"files/tsnelearning.png\">\n",
    "\n",
    "Lastly when clustering the points on the lower dimension, it uses a T-Distribution instead of the normal Gausian distribution that we used earlier. This is where the \"T\" in T-SNE comes from:\n",
    "\n",
    "<img src=\"files/tdistribution.png\">\n",
    "\n",
    "Source: https://www.youtube.com/watch?v=NEaUSP4YerM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_tsne = TSNE(learning_rate=100).fit_transform(V.toarray())\n",
    "\n",
    "print \"Shape of V_tsne: \", V_tsne.shape\n",
    "\n",
    "x = V_tsne[:,0]\n",
    "y = V_tsne[:,1]\n",
    "\n",
    "points = V_tsne[:,0:0]\n",
    "color = np.sqrt((points**2).sum(axis=1))/np.sqrt(2.0)\n",
    "rgb = plt.get_cmap('jet')(color)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.scatter(x,y,color=rgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above reduced the original vector V into 2 dimensional vector that we can then plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Shape of V: (99, 1430)\n",
    "Shape of V_tsne:  (99, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ploting the above V_tsne vector, we are presented with a somewhat evenly distributed plot:\n",
    "\n",
    "<img src=\"files/plot1.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2-2: Performing K-Means Clustering\n",
    "\n",
    "For the next part of the project, I had decided to perform K-Means clustering to start grouping my documents into similar topics.\n",
    "\n",
    "For this, I had decided to continue using the sklearn library, and use it's KMeans functionality to help me perform K-Mean clustering on my previously obtained TF-IDF vector of all the documents in my corpus.\n",
    "\n",
    "#### How K-Means Clustering Work?\n",
    "\n",
    "Input: takes in a set of points in euclidean space (x1...xn)\n",
    "Places initial k number of centroids (c1...ck)\n",
    "Repeat until convergence of all points onto the centroids:\n",
    "    - for each point x:\n",
    "        * Find nearest centroid\n",
    "        * Assign the point to the centroid's cluster\n",
    "    - for each cluster\n",
    "        * Assign new centroid based on the average distance of all points in the cluster\n",
    "\n",
    "\n",
    "<img src=\"files/kmeans_info.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# K-Means Clustering\n",
    "#\n",
    "\n",
    "km = KMeans(n_clusters=7, init='k-means++', n_init=10, max_iter=300)\n",
    "km.fit(V)\n",
    "\n",
    "print km.predict(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the above function KMeans from our sklearn library, we are able to provide parmeters such as number of clusters, and maximum number of iterations to try and achieve convergence. Below are the results of the above clustering where number of clusters n is set to 7.\n",
    "\n",
    "We can see that the array of 99 documents are clustered into 7 distinct (0...6) clusters. On different runs, I noticed the documents were sometimes clustered slightly differently. I believe I will need to further fine tune the numbr of clusters I pick and compare the results by checking each documents within the same cluster for similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[5 5 5 5 5 3 3 3 3 3 0 3 3 3 4 3 5 3 6 1 6 1 4 1 4 6 1 0 2 1 6 1 6 1 1 4 2\n",
    " 6 6 4 6 4 1 6 2 2 1 4 5 6 5 6 3 4 1 6 6 6 2 0 2 1 6 2 2 1 1 1 0 1 4 4 0 0\n",
    " 1 0 1 3 4 0 3 6 0 4 1 1 1 6 6 2 1 1 4 1 4 6 1 1 1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
